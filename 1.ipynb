{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this variable will be used in `runner.train` and by default we disable FP16 mode\n",
    "is_fp16_used = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catalyst\n",
    "!pip install catalyst==22.4\n",
    "\n",
    "# for augmentations\n",
    "!pip install albumentations==0.4.3\n",
    "\n",
    "# for pretrained segmentation models for PyTorch\n",
    "!pip install segmentation-models-pytorch\n",
    "\n",
    "# for TTA\n",
    "!pip install ttach==0.0.2\n",
    "\n",
    "# for tensorboard\n",
    "!pip install tensorflow\n",
    "\n",
    "# if Your machine support Apex FP16, uncomment this 3 lines below\n",
    "# !git clone https://github.com/NVIDIA/apex\n",
    "# !pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n",
    "# is_fp16_used = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import catalyst\n",
    "from catalyst import utils\n",
    "\n",
    "print(f\"torch: {torch.__version__}, catalyst: {catalyst.__version__}\")\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # \"\" - CPU, \"0\" - 1 GPU, \"0,1\" - MultiGPU\n",
    "\n",
    "SEED = 42\n",
    "utils.set_global_seed(SEED)\n",
    "utils.prepare_cudnn(deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "ROOT = Path(\"Dataset/\")\n",
    "\n",
    "train_image_path = os.path.join(ROOT, \"train/images\")\n",
    "train_mask_path = os.path.join(ROOT, \"train/masks\")\n",
    "test_image_path = os.path.join(ROOT, \"test/images\")\n",
    "test_mask_path = os.path.join(ROOT, \"test/masks\")\n",
    "valid_image_path = os.path.join(ROOT, \"val/images\")\n",
    "valid_mask_path = os.path.join(ROOT, \"val/masks\")\n",
    "\n",
    "ALL_IMAGES = sorted(os.listdir(train_image_path))\n",
    "ALL_MASKS = sorted(os.listdir(train_mask_path))\n",
    "ALL_TEST_IMAGES = sorted(os.listdir(test_image_path))\n",
    "ALL_VALID_IMAGES = sorted(os.listdir(valid_image_path))\n",
    "ALL_VALID_MASKS = sorted(os.listdir(valid_mask_path))\n",
    "ALL_TEST_MASKS = sorted(os.listdir(test_mask_path))\n",
    "print(f\"Number of train images/masks: {len(ALL_IMAGES)}\")\n",
    "print(f\"Number of test images: {len(ALL_TEST_IMAGES)}\")\n",
    "print(f\"Number of valid images/masks: {len(ALL_VALID_IMAGES)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.io import imread as gif_imread\n",
    "from catalyst import utils\n",
    "\n",
    "\n",
    "def show_examples(name: str, image: np.ndarray, mask: np.ndarray):\n",
    "    plt.figure(figsize=(10, 14))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Image: {name}\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask)\n",
    "    plt.title(f\"Mask: {name}\")\n",
    "\n",
    "\n",
    "def show(index: int, images: List[Path], masks: List[Path], transforms=None) -> None:\n",
    "\n",
    "    image = gif_imread(\"Dataset/train/images/\" + images[index])\n",
    "    mask = gif_imread(\"Dataset/train/masks/\" + masks[index])\n",
    "    name = images[index]\n",
    "\n",
    "    if transforms is not None:\n",
    "        temp = transforms(image=image, mask=mask)\n",
    "        image = temp[\"image\"]\n",
    "        mask = temp[\"mask\"]\n",
    "\n",
    "    show_examples(name, image, mask)\n",
    "\n",
    "def show_random(images: List[Path], masks: List[Path], transforms=None) -> None:\n",
    "    length = len(images)\n",
    "    index = random.randint(0, length - 1)\n",
    "    show(index, images, masks, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images: List[Path],\n",
    "        masks: List[Path] = None,\n",
    "        transforms=None,\n",
    "        data_type: str = \"train\",\n",
    "    ) -> None:\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.transforms = transforms\n",
    "        self.data_type = data_type\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        image_path = self.images[idx]\n",
    "        image = gif_imread(\"Dataset/\" + self.data_type + \"/images/\" + image_path)\n",
    "        # Transpose the image to change its shape\n",
    "        transposed_image = np.transpose(image, (2, 0, 1))\n",
    "        image = transposed_image\n",
    "        result = {\"image\": image}\n",
    "        \n",
    "        if self.masks is not None:\n",
    "            mask = gif_imread(\"Dataset/\" + self.data_type + \"/masks/\" + self.masks[idx])\n",
    "            result[\"mask\"] = mask\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            result = self.transforms(**result)\n",
    "        \n",
    "        result[\"filename\"] = image_path\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2 as ToTensor\n",
    "\n",
    "\n",
    "def pre_transforms(image_size=128):\n",
    "    return [albu.Resize(image_size, image_size, p=1)]\n",
    "\n",
    "\n",
    "def hard_transforms():\n",
    "    result = [\n",
    "      albu.RandomRotate90(),\n",
    "      albu.Cutout(),\n",
    "      albu.RandomBrightnessContrast(\n",
    "          brightness_limit=0.2, contrast_limit=0.2, p=0.3\n",
    "      ),\n",
    "      albu.GridDistortion(p=0.3),\n",
    "      albu.HueSaturationValue(p=0.3)\n",
    "    ]\n",
    "\n",
    "    return result\n",
    "  \n",
    "\n",
    "def resize_transforms(image_size=128):\n",
    "    BORDER_CONSTANT = 0\n",
    "    pre_size = image_size \n",
    "\n",
    "    random_crop = albu.Compose([\n",
    "      albu.SmallestMaxSize(pre_size, p=1),\n",
    "      albu.RandomCrop(\n",
    "          image_size, image_size, p=1\n",
    "      )\n",
    "\n",
    "    ])\n",
    "\n",
    "    rescale = albu.Compose([albu.Resize(image_size, image_size, p=1)])\n",
    "\n",
    "    random_crop_big = albu.Compose([\n",
    "      albu.LongestMaxSize(pre_size, p=1),\n",
    "      albu.RandomCrop(\n",
    "          image_size, image_size, p=1\n",
    "      )\n",
    "\n",
    "    ])\n",
    "\n",
    "    # Converts the image to a square of size image_size x image_size\n",
    "    result = [\n",
    "      albu.OneOf([\n",
    "          random_crop,\n",
    "          rescale,\n",
    "          random_crop_big\n",
    "      ], p=1)\n",
    "    ]\n",
    "\n",
    "    return result\n",
    "  \n",
    "def post_transforms():\n",
    "    # we use ImageNet image normalization\n",
    "    # and convert it to torch.Tensor\n",
    "    return [albu.Normalize(), ToTensor()]\n",
    "  \n",
    "def compose(transforms_to_compose):\n",
    "    # combine all augmentations into single pipeline\n",
    "    result = albu.Compose([\n",
    "      item for sublist in transforms_to_compose for item in sublist\n",
    "    ])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),    # Resize images to 128x128 pixels\n",
    "    transforms.ToTensor(),            # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet statistics\n",
    "])\n",
    "\n",
    "train_transforms = compose([resize_transforms()])\n",
    "valid_transforms = compose([resize_transforms()])\n",
    "\n",
    "show_transforms = compose([resize_transforms(), hard_transforms()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random(ALL_IMAGES, ALL_MASKS, transforms=show_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_fp16_used:\n",
    "    batch_size = 32\n",
    "else:\n",
    "    batch_size = 8\n",
    "\n",
    "print(f\"batch_size: {batch_size}\")\n",
    "train_dataset=SegmentationDataset(\n",
    "    images=ALL_IMAGES,\n",
    "    masks=ALL_MASKS,\n",
    "    transforms=train_transforms,\n",
    "    data_type=\"train\"\n",
    ")\n",
    "valid_dataset=SegmentationDataset(\n",
    "    images=ALL_VALID_IMAGES,\n",
    "    masks=ALL_VALID_MASKS,\n",
    "    transforms=valid_transforms,\n",
    "    data_type=\"val\"\n",
    ")\n",
    "test_dataset=SegmentationDataset(\n",
    "    images=ALL_TEST_IMAGES,\n",
    "    masks=ALL_TEST_MASKS,\n",
    "    transforms=valid_transforms,\n",
    "    data_type=\"test\"\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    ")\n",
    "print(\"len(train_dataset):\", len(train_dataset))\n",
    "print(\"len(valid_dataset):\", len(valid_dataset))\n",
    "print(\"len(train_loader):\", len(train_loader))\n",
    "print(\"len(valid_loader):\", len(valid_loader))\n",
    "    \n",
    "loaders = collections.OrderedDict()\n",
    "loaders[\"train\"] = train_loader\n",
    "loaders[\"valid\"] = valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.utils.losses import DiceLoss\n",
    "# We will use Feature Pyramid Network with pre-trained ResNeXt50 backbone\n",
    "model = smp.FPN(encoder_name=\"resnext50_32x4d\", classes=1)\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# we have multiple criterions\n",
    "criterion = {\n",
    "    \"dice\": DiceLoss(),\n",
    "    \"bce\": nn.BCEWithLogitsLoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "encoder_learning_rate = 0.0005\n",
    "\n",
    "# Since we use a pre-trained encoder, we will reduce the learning rate on it.\n",
    "layerwise_params = {\"encoder*\": dict(lr=encoder_learning_rate, weight_decay=0.00003)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import SupervisedRunner\n",
    "\n",
    "num_epochs = 3\n",
    "logdir = \"./logs/segmentation\"\n",
    "\n",
    "device = utils.get_device()\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "if is_fp16_used:\n",
    "    fp16_params = dict(opt_level=\"O1\") # params for FP16\n",
    "else:\n",
    "    fp16_params = None\n",
    "\n",
    "print(f\"FP16 params: {fp16_params}\")\n",
    "\n",
    "\n",
    "# by default SupervisedRunner uses \"features\" and \"targets\",\n",
    "# in our case we get \"image\" and \"mask\" keys in dataset __getitem__\n",
    "\n",
    "runner = SupervisedRunner(input_key=\"image\", target_key=\"mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import \\\n",
    "  CriterionCallback, MetricAggregationCallback\n",
    "\n",
    "\n",
    "from catalyst.callbacks.metrics.segmentation import DiceCallback,IOUCallback\n",
    "\n",
    "callbacks = [\n",
    "    # Each criterion is calculated separately.\n",
    "    CriterionCallback(\n",
    "        input_key=\"mask\",\n",
    "        prefix=\"loss_dice\",\n",
    "        criterion_key=\"dice\",\n",
    "        target_key=\"mask\",\n",
    "        metric_key=\"dice\"\n",
    "    ),\n",
    "\n",
    "    CriterionCallback(\n",
    "        input_key=\"mask\",\n",
    "        prefix=\"loss_bce\",\n",
    "        criterion_key=\"bce\",\n",
    "        target_key=\"mask\",\n",
    "        metric_key=\"bce\"\n",
    "    ),\n",
    "\n",
    "    # And only then we aggregate everything into one loss.\n",
    "    MetricAggregationCallback(\n",
    "        metric_key=\"loss\",\n",
    "        mode=\"weighted_sum\", # can be \"sum\", \"weighted_sum\" or \"mean\"\n",
    "        # because we want weighted sum, we need to add scale for each loss\n",
    "        metrics={\"loss_dice\": 1.0, \"loss_iou\": 1.0, \"loss_bce\": 0.8},\n",
    "    ),\n",
    "\n",
    "    # metrics\n",
    "    DiceCallback(input_key=\"image\", target_key=\"mask\"),\n",
    "    IOUCallback(input_key=\"image\", target_key=\"mask\"),\n",
    "\n",
    "]\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    # our dataloaders\n",
    "    loaders=loaders,\n",
    "    # We can specify the callbacks list for the experiment;\n",
    "    callbacks=callbacks,\n",
    "    # path to save logs\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    # IoU needs to be maximized.\n",
    "    valid_metric=\"dice\",\n",
    "    minimize_valid_metric=False,\n",
    "    # for FP16. It uses the variable from the very first cell\n",
    "    fp16=fp16_params,\n",
    "    # prints train logs\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGES = sorted(test_image_path.glob(\"*.jpg\"))\n",
    "\n",
    "# create test dataset\n",
    "test_dataset = SegmentationDataset(\n",
    "    TEST_IMAGES, \n",
    "    transforms=valid_transforms\n",
    ")\n",
    "\n",
    "num_workers: int = 4\n",
    "\n",
    "infer_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "# this get predictions for the whole loader\n",
    "predictions = np.vstack(list(map(\n",
    "    lambda x: x[\"logits\"].cpu().numpy(), \n",
    "    runner.predict_loader(loader=infer_loader, resume=f\"{logdir}/checkpoints/best.pth\")\n",
    ")))\n",
    "\n",
    "print(type(predictions))\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "max_count = 5\n",
    "\n",
    "for i, (features, logits) in enumerate(zip(test_dataset, predictions)):\n",
    "    image = utils.tensor_to_ndimage(features[\"image\"])\n",
    "\n",
    "    mask_ = torch.from_numpy(logits[0]).sigmoid()\n",
    "    mask = utils.detach(mask_ > threshold).astype(\"float\")\n",
    "        \n",
    "    show_examples(name=\"\", image=image, mask=mask)\n",
    "    \n",
    "    if i >= max_count:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loaders[\"valid\"]))\n",
    "# saves to `logdir` and returns a `ScriptModule` class\n",
    "runner.trace(model=model, batch=batch, logdir=logdir, fp16=is_fp16_used)\n",
    "\n",
    "!ls {logdir}/trace/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.utils import trace\n",
    "\n",
    "if is_fp16_used:\n",
    "    model = trace.load_traced_model(\n",
    "        f\"{logdir}/trace/traced-forward-opt_O1.pth\", \n",
    "        device=\"cuda\", \n",
    "        opt_level=\"O1\"\n",
    "    )\n",
    "else:\n",
    "    model = trace.load_traced_model(\n",
    "        f\"{logdir}/trace/traced-forward.pth\", \n",
    "        device=\"cpu\"\n",
    "    )\n",
    "\n",
    "model_input = batch[\"image\"].to(\"cuda\" if is_fp16_used else \"cpu\")\n",
    "model(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
